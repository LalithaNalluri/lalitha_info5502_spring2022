{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third Lab-assignment (02/10/2022, 50 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nDomain problem:  To find out the movie which has IMDB ratings high.\\nResearch Question: To get movies data using the IMDB website\\nIn this we get movies with high ratings, hence we can understand whether if the movie is good or not based on its ratings.\\n\\nData needed to answer the questions:\\n\\nThe attributes present are:\\n. MovieName\\n. ReleaseYear\\n. Ratings\\n. RunTime\\n\\n\\nThe data is retrived from the link:\\nhttps://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&start=\\n\\n\\nThe steps that are involved in the process to collect and store data:\\n1. We use web scraping for scraping the data invloved, by using BeautifulSoup library in the IMDB website that contains several pages\\n2. Every page is scraped and the information required is retrived.\\n3. We get the Movie name, ratings, release year and runtime details after we inspect the page on click on its HTML script\\n4. We then retrive the data by parsing through the details present and then copy them to csv format.\\n5. Finally, we get the data in the xlsx format after performing the web scrap.\\n\\n\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "''''\n",
    "Domain problem:  To find out the movie which has IMDB ratings high.\n",
    "Research Question: To get movies data using the IMDB website\n",
    "In this we get movies with high ratings, hence we can understand whether if the movie is good or not based on its ratings.\n",
    "\n",
    "\n",
    "The attributes present are:\n",
    ". MovieName\n",
    ". ReleaseYear\n",
    ". Ratings\n",
    ". RunTime\n",
    "\n",
    "\n",
    "The data is retrived from the link:\n",
    "https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&start=\n",
    "\n",
    "\n",
    "The steps that are involved in the process to collect and store data:\n",
    "1. We use web scraping for scraping the data invloved, by using BeautifulSoup library in the IMDB website that contains several pages\n",
    "2. Every page is scraped and the information required is retrived.\n",
    "3. We get the Movie name, ratings, release year and runtime details after we inspect the page on click on its HTML script\n",
    "4. We then retrive the data by parsing through the details present and then copy them to csv format.\n",
    "5. Finally, we get the data in the xlsx format after performing the web scrap.\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (10 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DataFrame: 1000\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "listValues=[] #creating a list to store the movie details\n",
    "\n",
    "for i in range(0,10):  #giving range 1000 \n",
    "  html_Text=requests.get('https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&count=100&start='+str(i)+'01&ref_=adv_nxt').text\n",
    "  soup=bs(html_Text,'lxml')  #parsing html using BeautifulSoup and lxml\n",
    "  movies=soup.find_all('div',class_='lister-item mode-advanced')\n",
    "\n",
    "  for movie in movies: \n",
    "    movieName=movie.find('span',class_='lister-item-index unbold text-primary').text+ movie.find('h3',class_='lister-item-header').a.text #Extracting movie name from page by filtering with the class 'lister-item-index unbold text-primary'\n",
    "    releaseYear=movie.find('span',class_='lister-item-year text-muted unbold').text.replace('(','').replace(')','') #Extracting release year from page by filtering with the class 'lister-item-year text-muted unbold'\n",
    "    runTime=movie.find('span',class_='runtime').text #Extracting run time from page by filtering with the class 'runtime'\n",
    "    rating=movie.find('div',class_='inline-block ratings-imdb-rating').strong.text #Extracting rating from page by filtering with the class 'inline-block ratings-imdb-rating'\n",
    "    listValues.append([movieName,releaseYear,runTime,rating]) #appending movieName,releaseYear,runTime and rating to the list\n",
    "\n",
    "df=pd.DataFrame(listValues,columns=['MovieName','ReleaseYear','RunTime','Rating']) #creating a data frame using list which has all the movie details\n",
    "print(\"Length of DataFrame:\",len(df))\n",
    "df.to_csv('movie.csv',index= False ,header = False)#1000 top imdb movies will be scrapped  into movie.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Using the SuperMarket sales data from Kaggle.\n",
    "Dataset link : https://www.kaggle.com/aungpyaeap/supermarket-sales\n",
    "The dataset contains attributes date, Branch, Cogs and others\n",
    "\n",
    "Steps invloved in cleaning the data:\n",
    "1. Converting the attribute dates in appropriate format.\n",
    "2. In the dataframe removing all the unnecessary rows.\n",
    "3. Then, the dataframe index is changed.\n",
    "4. Converting and cleaning the columns which are numericals to Strings using .str() function.\n",
    "5. In the data frame clean the entire dataset element wise by using applymap()\n",
    "6. To skip the rows that aren't necessary and removing all the stop words from the csv data\n",
    "7. Then, removing any spaces that aren't required and also removing special characters.\n",
    "8. To check if any Null or blank data is present.\n",
    "9. If any imbalanced data present, we balance it by using SMOTE technique which performs over and under sampling.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 (20 points). Data cleaning: There are two datasets TwADR-L (from Twitter) and AskAPatient (Link: https://zenodo.org/record/55013#.YgU2NN-ZO4T) for medical concept\n",
    "normalization. However, the two datasets have serious data quality problems. Please read the introduction of the datasets and clean the two datasets by following the steps in the statement.\n",
    "\n",
    "In the original dataset, the TwADR-L had 48,057 training, 1,256 validation and 1,427 test examples. The test set (all\n",
    "test samples from 10 folds combined) consists of 765 unique phrases and 273 unique classes (medical concepts). The AskAPatient dataset contained 156,652 training, 7,926 validation, and 8,662 test examples. The entire test set (all test samples\n",
    "from 10 folds combined) consists of 3,749 unique phrases and 1,035 unique classes (medical concepts). The authors\n",
    "randomly split each dataset into ten equal folds, ran 10-fold cross validation and reported the accuracy averaged across the\n",
    "ten folds. \n",
    "\n",
    "We found that, in the original data set, many phrase-label pairs appeared multiple times within the same training data file\n",
    "and also across the training and test data sets in the same fold. In the AskAPatient data set, on average 35.82% of the test data overlapped with training data in the same fold. In the Twitter (TwADR-L) dataset, on average 8.62% of the test set had an overlap with the training data in the same fold. Having a large overlap between the training and the test data can potentially\n",
    "introduce bias in the model and contribute to high accuracy. It is not unlikely that the high model performance reported in the original paper may be triggered by the the large overlap between the training and test sets.\n",
    "\n",
    "Therefore to remove the bias, we further cleaned and recreated the training, validation, and test sets such that each\n",
    "phrase-label pair appears only once in the entire dataset (either in training, validation or test set).\n",
    "\n",
    "(1) First, we combined all examples in training, validation and test data from the original data set and then removed all\n",
    "duplicate phrase-label pairs (examples that have the same phrase and label pair and appear more than once in training/validation/test datasets). Table II shows the statistics of the new dataset (after removing duplicates). The Twitter data set had 3,157 unique phrase-label pairs and 2,220 unique labels (medical concepts) while 173 phrases had multiple labels (i.e., they were assigned to more than one label). Many concepts had only one example, and the concept that had the most number\n",
    "of examples had 36 phrases. On average, each concept had 1.42 examples. The AskAPatient data set had 4,496 unique phrase-label pairs, 1,036 unique labels while 26 phrases had multiple labels. Table III shows examples of phrases that had multiple labels. For example, ‘mad’ can be mapped to ‘anger’ or ‘rage’ and ‘sore’ can be mapped to ‘pain’ or ‘myalgia’.\n",
    "\n",
    "(2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
    "\n",
    "(3) Third, we divide all examples without multiple labels into random 10 folds such that each unique phrase-label pair\n",
    "appears once in one of the 10 test sets. We add the pairs with multiple labels into the training data. This final 10-folds\n",
    "dataset is used in all our experiments.\n",
    "\n",
    "(The original paper can be download on canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TwADR-L</th>\n",
       "      <th>AskAPatient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#Unique phrases</th>\n",
       "      <td>2220</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Unique labels</th>\n",
       "      <td>2944</td>\n",
       "      <td>4470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># unique phrase-label pairs</th>\n",
       "      <td>3157</td>\n",
       "      <td>4507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># phrases with multiple labels</th>\n",
       "      <td>173</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min # examples per label</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max # examples per label</th>\n",
       "      <td>36</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg # examples per label</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                TwADR-L  AskAPatient\n",
       "#Unique phrases                    2220         1038\n",
       "#Unique labels                     2944         4470\n",
       "# unique phrase-label pairs        3157         4507\n",
       "# phrases with multiple labels      173           35\n",
       "Min # examples per label              1            1\n",
       "Max # examples per label             36          141\n",
       "Avg # examples per label              1            4"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "dataset=['TwADR-L','AskAPatient']\n",
    "\n",
    "result_df=pd.DataFrame()\n",
    "path='C:/Users/lalit/Desktop/5502/datasets'\n",
    "#print(path)\n",
    "for name in dataset:\n",
    "    result={}\n",
    "    df=pd.DataFrame()\n",
    "    file=path+'/'+name\n",
    "    #print(name,file)\n",
    "    os.chdir(file)\n",
    "    os.chmod(file,0o777)\n",
    "    for files in glob.glob('*.txt'):\n",
    "        #print (files)\n",
    "        rd=pd.read_csv(files,sep='\\t',header = None, encoding= 'unicode_escape')\n",
    "        df=df.append(rd)\n",
    "    df.reset_index(drop=True)\n",
    "    df.columns=['ID','label','Phrase']\n",
    "    df['label']=df['label'].str.upper()\n",
    "    df['Phrase']=df['Phrase'].str.upper()\n",
    "    df.drop_duplicates(subset=['label','Phrase'],inplace=True)\n",
    "    df[\"Phrase-label\"]=df['Phrase']+df['label']\n",
    "    result['#Unique phrases'] =df['label'].nunique()\n",
    "    result['#Unique labels'] =len(df['Phrase'].unique())\n",
    "    result['# unique phrase-label pairs'] =df[\"Phrase-label\"].nunique()\n",
    "    #occur = df.groupby(['label']).size()\n",
    "    #print(occur[1]>1)#['label']>1)\n",
    "    df1 = pd.DataFrame(df['Phrase'].value_counts())\n",
    "    result[\"# phrases with multiple labels\"]=df1[df1['Phrase'] > 1].shape[0]\n",
    "    result['Min # examples per label'] = df['label'].value_counts().values.min()\n",
    "    result['Max # examples per label'] = df['label'].value_counts().values.max()\n",
    "    result['Avg # examples per label'] = round(df['label'].value_counts().mean(), 2)\n",
    "    result_df = result_df.append(result, ignore_index=True)\n",
    "    #Array split\n",
    "    print(np.array_split(df,10))\n",
    "\n",
    "result_df = result_df.T\n",
    "result_df = result_df.astype({0: int, 1: int})\n",
    "result_df.columns = ['TwADR-L', 'AskAPatient']\n",
    "\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TwADR-L</th>\n",
       "      <th>AskAPatient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#Unique phrases</th>\n",
       "      <td>1038</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Unique labels</th>\n",
       "      <td>4470</td>\n",
       "      <td>4470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># unique phrase-label pairs</th>\n",
       "      <td>4507</td>\n",
       "      <td>4507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># phrases with multiple labels</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min # examples per label</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max # examples per label</th>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg # examples per label</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                TwADR-L  AskAPatient\n",
       "#Unique phrases                    1038         1038\n",
       "#Unique labels                     4470         4470\n",
       "# unique phrase-label pairs        4507         4507\n",
       "# phrases with multiple labels       35           35\n",
       "Min # examples per label              1            1\n",
       "Max # examples per label            141          141\n",
       "Avg # examples per label              4            4"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1_df=pd.DataFrame()\n",
    "for name in dataset:\n",
    "    result1={}\n",
    "    index_list = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if df['label'].value_counts()[df.iloc[i]['label']] < 5:\n",
    "            index_list.append(i)\n",
    "    df.drop(df.index[index_list],inplace=True)\n",
    "    result1['#Unique phrases'] =df['label'].nunique()\n",
    "    result1['#Unique labels'] =len(df['Phrase'].unique())\n",
    "    result1['# unique phrase-label pairs'] =df[\"Phrase-label\"].nunique()\n",
    "    #occur = df.groupby(['label']).size()\n",
    "    #print(occur[1]>1)#['label']>1)\n",
    "    df1 = pd.DataFrame(df['Phrase'].value_counts())\n",
    "    result1[\"# phrases with multiple labels\"]=df1[df1['Phrase'] > 1].shape[0]\n",
    "    result1['Min # examples per label'] = df['label'].value_counts().values.min()\n",
    "    result1['Max # examples per label'] = df['label'].value_counts().values.max()\n",
    "    result1['Avg # examples per label'] = round(df['label'].value_counts().mean(), 2)\n",
    "    result1_df = result1_df.append(result1, ignore_index=True)\n",
    "\n",
    "\n",
    "result1_df = result1_df.T\n",
    "result1_df = result1_df.astype({0: int, 1: int})\n",
    "result1_df.columns = ['TwADR-L', 'AskAPatient']\n",
    "\n",
    "result1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
